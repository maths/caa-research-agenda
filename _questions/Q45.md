---
question_code: Q45 
question_num: 45 
question_text: To what extent can current e-assessments meaningfully judge student responses to example generation tasks? 

question_code_meeting1: B9 
question_code_conf: TD4 

contributors: 
 - georgekinnear
 - sangwinc

---

## What motivates this question?

Learner-generated example tasks have been suggested as an effective way to encourage students to engage with new concepts (Watson & Mason, 2006). Some CAA systems appear well-placed to facilitate this type of task, as they can make use of sophisticated computer algebra systems to check the properties of student examples - however this relies on the properties being ones that the system can evaluate. For instance, if asking for functions which tend to 0 at infinity, the computer algebra system would need to have the ability to evaluate limits.

## What might an answer look like?

This would involve a mix of design work to identify suitable topics and questions, and empirical work to gather typical student responses. For instance, in the limits example: while it may not technically be possible for the computer algebra system to evaluate the limit of an arbitrary function, it is likely that in practice students will only give a range of reasonably well-behaved examples which may remain within the ability of computer algebra system to evaluate.

An output could be a set of exemplar questions resulting from cycles of design research (Swan, 2014).

## Related questions

* For responses that cannot be automatically judged, there could be some human intervention: [Q47](Q47), [Q48](Q48), [Q49](Q49).
* The student responses may be constrained by their ability to enter them into the system [Q50](Q50).

## References

Swan M. (2014) Design Research in Mathematics Education. In: Lerman S. (eds) Encyclopedia of Mathematics Education. Springer, Dordrecht. https://doi.org/10.1007/978-94-007-4978-8_180

Watson, A., & Mason, J. (2006). Mathematics as a Constructive Activity. Routledge. https://doi.org/10.4324/9781410613714
