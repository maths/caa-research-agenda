---
question_code: Q53 
question_num: 53 
question_text: "How do students engage with automated feedback: what differences (if any) can be identified with how they would respond to feedback from a teacher?"

question_code_meeting1:  
question_code_conf: NEW1 

contributors: 
- ikon625
- prowlett
- niclaslarson
- georgekinnear

---

## What motivates this question?

How students engage with feedback and what they do next, whether it promotes positive learning behaviour, is clearly important to close the loop of assessment for learning. Yet apparently little is known about what students do when they get feedback. Do they read it and engage with it? What do they do next? If they got a question wrong, do they attempt relevant learning and reattempt the assessment item? If they get positive feedback, do they set themselves new learning goals to advance to the next level of the topic? 

A recent "critical scoping review" of research on student perceptions of assessment feedback (Van der Kleij and Lipnevich, 2020) provides useful background information. Out of 164 studies included in the review, only 4 were based in mathematics. Similarly, from the summary of each study presented in the supplementary materials, a search for "computer" suggested that very few of the included studies were related to automated feedback (studies 43, 89, 90, 148).

## What might an answer look like?

Some different approaches are possible:

- A questionnaire asking students to report their behaviour could be used.
- It could be that tracking within an integrated learning and assessment system could be used to track what students do when engaging with feedback and what they do afterwards.
- Direct observation of students as they work through tasks could give insights that are not possible from self-reports or from analytics data (e.g., Jordan, 2012).

The framework presented by Van der Kleij and Lipnevich (2020, Fig. 3) could also inform the selection of methods, particularly as they highlight methodological concerns with survey-based research.

## Related questions

* The issue of student engagement with automated feedback overlaps with [Q35: How do mathematics students interact with an e-assessment system?](Q35)
* The issue of relevance of feedback is relevant to [Q6: How can content-specific features of provided feedback, for instance explanations with examples versus generic explanations, support students' learning?](Q6).
* Some discussion of student preference re. automated and human feedback is given in [Q13: In what circumstances is instant feedback from automated marking preferable to marking by hand?](Q13).
* The way students engage with feedback may be different if the e-assessment system gives feedback in a way that emulates teacher approaches, as in:
  - [Q10: How can feedback that is dynamically tailored to the student’s level of mathematical expertise help a student use feedback on mathematical tasks effectively?](Q10)
  - [Q11: How useful for students’ long-term learning is feedback that gives a series of follow-up questions, from a decision tree, versus a single terminal piece of feedback that tells the students exactly what they should have done?](Q11)
  - [Q12: What are the relative benefits of e-assessment giving feedback on a student’s set of responses (e.g. “two of these answers are wrong – find which ones and correct them”), rather than individual ones separately?](Q12)
  - [Q33: What should students be encouraged to do following success in e-assessment?](Q33)
  - [Q34: Does repeated practice on similar problems encourage mathematics students to discover deep links between items, or simply encourage memorisation and pattern-spotting?](Q34)


## References

Jordan, S. (2012). Student engagement with assessment and feedback: some lessons from short-answer free-text e-assessment questions. Computers & Education, 58(2), 818-834. https://doi.org/10.1016/j.compedu.2011.10.007

Van der Kleij, F. M., & Lipnevich, A. A. (2020). Student perceptions of assessment feedback: a critical scoping review and call for research. Educational Assessment, Evaluation and Accountability, 33(2), 345–373. https://doi.org/10.1007/s11092-020-09331-x
